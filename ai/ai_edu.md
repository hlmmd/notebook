# ai_edu

## basic knowledge

### 第2章 神经网络中的三个基本概念

反向传播，梯度下降，损失函数

均方差损失函数

交叉熵损失函数

## linearRegression

线性回归

最小二乘

梯度下降

神经网络

多样本计算

* 单样本随机梯度下降
* 小批量样本梯度下降
* 全批量样本梯度下降

### 多入单出的单层神经网络

正规方程解法

神经网络法

1. X必须标准化，否则无法训练；
2. Y值不在[0,1]之间时，要做标准化，好处是迭代次数少；
3. 如果Y做了标准化，对得出来的预测结果做关于Y的反标准化

1. 样本不做标准化的话，网络发散，训练无法进行；
2. 训练样本标准化后，网络训练可以得到结果，但是预测结果有问题；
3. 还原参数值后，预测结果正确，但是此还原方法并不能普遍适用；
4. 标准化测试样本，而不需要还原参数值，可以保证普遍适用；
5. 标准化标签值，可以使得网络训练收敛快，但是在预测时需要把结果反标准化，以便得到真实值。

## 线性分类

回归问题可以分为两类：线性回归和逻辑回归

逻辑回归，Logistic Regression

用线性回归模型的预测结果来逼近样本分类的对数几率。这就是为什么它叫做逻辑回归(logistic regression)，但其实是分类学习的方法。这种方法的优点如下：

- 直接对分类可能性建模，无需事先假设数据分布，避免了假设分布不准确所带来的问题；
- 不仅预测出类别，而是得到了近似的概率，这对许多需要利用概率辅助决策的任务很有用；
- 对率函数是任意阶可导的凸函数，有很好的数学性，许多数值优化算法都可以直接用于求取最优解。

多分类问题一共有三种解法：

一对一方式
每次先只保留两个类别的数据，训练一个分类器。

一对多方式
如图7-4，处理一个类别时，暂时把其它所有类别看作是一类，这样对于三分类问题，可以得到三个分类器。

多对多方式
假设有4个类别ABCD，我们可以把AB算作一类，CD算作一类，训练一个分类器1；再把AC算作一类，BD算作一类，训练一个分类器2。

sigmoid

softmax分类函数

## 非线性回归

激活函数的基本性质：

+ 非线性：线性的激活函数和没有激活函数一样；
+ 可导性：做误差反向传播和梯度下降，必须要保证激活函数的可导性；
+ 单调性：单一的输入会得到单一的输出，较大值的输入得到较大值的输出。

1. Sigmoid，指的是对数几率函数用于激活函数时的称呼；
2. Logistic，指的是对数几率函数用于二分类函数时的称呼；
3. Tanh，指的是双曲正切函数用于激活函数时的称呼。

万能近似定理


## 非线性分类

## 模型的推理与部署

## 深度神经网络 DNN

## 卷积神经网络 CNN

## 循环神经网络 RNN